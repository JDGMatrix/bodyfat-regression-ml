{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12934504,"sourceType":"datasetVersion","datasetId":8184959}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"6ed4c134-6134-45f1-8ca1-d65e7b3e6f56","cell_type":"markdown","source":"# 02 â€” Preprocessing\n\n**Goal:** Build a reproducible preprocessing pipeline with scikit-learn.\n\nNow that we have gathered considerable insights regarding our dataset, we will proceed to use them for data preprocessing\n\n**Checklist**\n- Train/test split\n- Scale numeric features.\n- feature selection","metadata":{}},{"id":"7fc4901a-4e88-4843-82a1-41cf1ab7e6b4","cell_type":"code","source":"# Imports\nimport pandas as pd, numpy as np\nfrom scipy.io import arff\n\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nimport joblib\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T12:08:43.567839Z","iopub.execute_input":"2025-09-03T12:08:43.568114Z","iopub.status.idle":"2025-09-03T12:08:43.572900Z","shell.execute_reply.started":"2025-09-03T12:08:43.568096Z","shell.execute_reply":"2025-09-03T12:08:43.572050Z"}},"outputs":[],"execution_count":40},{"id":"f58097c3-f9c7-4e0e-9992-62613ac481fe","cell_type":"code","source":"# Load data\nfile_path = '/kaggle/input/bodyfat/bodyfat.arff'\nbf_arff = arff.loadarff(file_path)\ndf = pd.DataFrame(bf_arff[0])\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T10:30:45.142350Z","iopub.execute_input":"2025-09-03T10:30:45.142628Z","iopub.status.idle":"2025-09-03T10:30:45.169563Z","shell.execute_reply.started":"2025-09-03T10:30:45.142608Z","shell.execute_reply":"2025-09-03T10:30:45.168854Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"   Density   Age  Weight  Height  Neck  Chest  Abdomen    Hip  Thigh  Knee  \\\n0   1.0708  23.0  154.25   67.75  36.2   93.1     85.2   94.5   59.0  37.3   \n1   1.0853  22.0  173.25   72.25  38.5   93.6     83.0   98.7   58.7  37.3   \n2   1.0414  22.0  154.00   66.25  34.0   95.8     87.9   99.2   59.6  38.9   \n3   1.0751  26.0  184.75   72.25  37.4  101.8     86.4  101.2   60.1  37.3   \n4   1.0340  24.0  184.25   71.25  34.4   97.3    100.0  101.9   63.2  42.2   \n\n   Ankle  Biceps  Forearm  Wrist  class  \n0   21.9    32.0     27.4   17.1   12.3  \n1   23.4    30.5     28.9   18.2    6.1  \n2   24.0    28.8     25.2   16.6   25.3  \n3   22.8    32.4     29.4   18.2   10.4  \n4   24.0    32.2     27.7   17.7   28.7  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Density</th>\n      <th>Age</th>\n      <th>Weight</th>\n      <th>Height</th>\n      <th>Neck</th>\n      <th>Chest</th>\n      <th>Abdomen</th>\n      <th>Hip</th>\n      <th>Thigh</th>\n      <th>Knee</th>\n      <th>Ankle</th>\n      <th>Biceps</th>\n      <th>Forearm</th>\n      <th>Wrist</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0708</td>\n      <td>23.0</td>\n      <td>154.25</td>\n      <td>67.75</td>\n      <td>36.2</td>\n      <td>93.1</td>\n      <td>85.2</td>\n      <td>94.5</td>\n      <td>59.0</td>\n      <td>37.3</td>\n      <td>21.9</td>\n      <td>32.0</td>\n      <td>27.4</td>\n      <td>17.1</td>\n      <td>12.3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0853</td>\n      <td>22.0</td>\n      <td>173.25</td>\n      <td>72.25</td>\n      <td>38.5</td>\n      <td>93.6</td>\n      <td>83.0</td>\n      <td>98.7</td>\n      <td>58.7</td>\n      <td>37.3</td>\n      <td>23.4</td>\n      <td>30.5</td>\n      <td>28.9</td>\n      <td>18.2</td>\n      <td>6.1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0414</td>\n      <td>22.0</td>\n      <td>154.00</td>\n      <td>66.25</td>\n      <td>34.0</td>\n      <td>95.8</td>\n      <td>87.9</td>\n      <td>99.2</td>\n      <td>59.6</td>\n      <td>38.9</td>\n      <td>24.0</td>\n      <td>28.8</td>\n      <td>25.2</td>\n      <td>16.6</td>\n      <td>25.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0751</td>\n      <td>26.0</td>\n      <td>184.75</td>\n      <td>72.25</td>\n      <td>37.4</td>\n      <td>101.8</td>\n      <td>86.4</td>\n      <td>101.2</td>\n      <td>60.1</td>\n      <td>37.3</td>\n      <td>22.8</td>\n      <td>32.4</td>\n      <td>29.4</td>\n      <td>18.2</td>\n      <td>10.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0340</td>\n      <td>24.0</td>\n      <td>184.25</td>\n      <td>71.25</td>\n      <td>34.4</td>\n      <td>97.3</td>\n      <td>100.0</td>\n      <td>101.9</td>\n      <td>63.2</td>\n      <td>42.2</td>\n      <td>24.0</td>\n      <td>32.2</td>\n      <td>27.7</td>\n      <td>17.7</td>\n      <td>28.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"id":"62ad31da-3e92-44a4-9c47-a0c8aa86fab8","cell_type":"markdown","source":"## Double checking for issues\n\nNormally and quite often, datasets may have certain issues that can cause problems later down the line when we train our models, but from our analysis in the `01_exploration.ipynb` notebook, we have observed that there are, thankfully, no issues at all across all our data. But for the sake of this step and verification, we will double-check if there aren't any issues at all.\n\nThe issues that we will be checking will be:\n- if any NA values are present\n- if there are any duplicate entries present\n\nAs for any outliers, we have gone through this during our previous notebook","metadata":{}},{"id":"2bb1101b-313b-4ef4-af89-708fc3b62110","cell_type":"code","source":"print(\"Missing values:\\n\", df.isnull().sum())\nprint(\"Duplicates:\", df.duplicated().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T10:45:10.224848Z","iopub.execute_input":"2025-09-03T10:45:10.225160Z","iopub.status.idle":"2025-09-03T10:45:10.237457Z","shell.execute_reply.started":"2025-09-03T10:45:10.225137Z","shell.execute_reply":"2025-09-03T10:45:10.236647Z"}},"outputs":[{"name":"stdout","text":"Missing values:\n Density    0\nAge        0\nWeight     0\nHeight     0\nNeck       0\nChest      0\nAbdomen    0\nHip        0\nThigh      0\nKnee       0\nAnkle      0\nBiceps     0\nForearm    0\nWrist      0\nclass      0\ndtype: int64\nDuplicates: 0\n","output_type":"stream"}],"execution_count":28},{"id":"e815c763-cfbc-4743-a9a3-47351f24cb1a","cell_type":"markdown","source":"## Splitting the target from the dataset\n\nAs it can be seen that the target we will be going with is the `'class'`, which contains the body fat percentages of the 252 individuals, we will be splitting it from the dataset and assigning it to `'y'` (standard naming convention for labels). The rest of the dataset will then be assigned to `'X'`.\n\nWe will also be dropping the `'Density'` column, as we have discussed during the previous notebook.","metadata":{}},{"id":"75ad5b50-f8c4-4fcb-a308-41309a92d067","cell_type":"code","source":"# Target & features\nTARGET = 'class'\nX = df.drop(columns=[TARGET, 'Density'])\ny = df[TARGET]\n\nX.head()\n#y.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T10:32:34.642464Z","iopub.execute_input":"2025-09-03T10:32:34.642780Z","iopub.status.idle":"2025-09-03T10:32:34.660763Z","shell.execute_reply.started":"2025-09-03T10:32:34.642758Z","shell.execute_reply":"2025-09-03T10:32:34.659936Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"    Age  Weight  Height  Neck  Chest  Abdomen    Hip  Thigh  Knee  Ankle  \\\n0  23.0  154.25   67.75  36.2   93.1     85.2   94.5   59.0  37.3   21.9   \n1  22.0  173.25   72.25  38.5   93.6     83.0   98.7   58.7  37.3   23.4   \n2  22.0  154.00   66.25  34.0   95.8     87.9   99.2   59.6  38.9   24.0   \n3  26.0  184.75   72.25  37.4  101.8     86.4  101.2   60.1  37.3   22.8   \n4  24.0  184.25   71.25  34.4   97.3    100.0  101.9   63.2  42.2   24.0   \n\n   Biceps  Forearm  Wrist  \n0    32.0     27.4   17.1  \n1    30.5     28.9   18.2  \n2    28.8     25.2   16.6  \n3    32.4     29.4   18.2  \n4    32.2     27.7   17.7  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Weight</th>\n      <th>Height</th>\n      <th>Neck</th>\n      <th>Chest</th>\n      <th>Abdomen</th>\n      <th>Hip</th>\n      <th>Thigh</th>\n      <th>Knee</th>\n      <th>Ankle</th>\n      <th>Biceps</th>\n      <th>Forearm</th>\n      <th>Wrist</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23.0</td>\n      <td>154.25</td>\n      <td>67.75</td>\n      <td>36.2</td>\n      <td>93.1</td>\n      <td>85.2</td>\n      <td>94.5</td>\n      <td>59.0</td>\n      <td>37.3</td>\n      <td>21.9</td>\n      <td>32.0</td>\n      <td>27.4</td>\n      <td>17.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22.0</td>\n      <td>173.25</td>\n      <td>72.25</td>\n      <td>38.5</td>\n      <td>93.6</td>\n      <td>83.0</td>\n      <td>98.7</td>\n      <td>58.7</td>\n      <td>37.3</td>\n      <td>23.4</td>\n      <td>30.5</td>\n      <td>28.9</td>\n      <td>18.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22.0</td>\n      <td>154.00</td>\n      <td>66.25</td>\n      <td>34.0</td>\n      <td>95.8</td>\n      <td>87.9</td>\n      <td>99.2</td>\n      <td>59.6</td>\n      <td>38.9</td>\n      <td>24.0</td>\n      <td>28.8</td>\n      <td>25.2</td>\n      <td>16.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26.0</td>\n      <td>184.75</td>\n      <td>72.25</td>\n      <td>37.4</td>\n      <td>101.8</td>\n      <td>86.4</td>\n      <td>101.2</td>\n      <td>60.1</td>\n      <td>37.3</td>\n      <td>22.8</td>\n      <td>32.4</td>\n      <td>29.4</td>\n      <td>18.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24.0</td>\n      <td>184.25</td>\n      <td>71.25</td>\n      <td>34.4</td>\n      <td>97.3</td>\n      <td>100.0</td>\n      <td>101.9</td>\n      <td>63.2</td>\n      <td>42.2</td>\n      <td>24.0</td>\n      <td>32.2</td>\n      <td>27.7</td>\n      <td>17.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"id":"c412ac9f-cdee-43c5-ab86-d226a08f8413","cell_type":"markdown","source":"## Splitting the dataset to train, validate and test datasets\n\nMoving on to one of the essential steps of data preprocessing. Now that we have defined the `'X'` (features) and `'y'` (target) values in our dataset, we will proceed to split them into two subsets: the **training** set and the **test** set (often I prefer to go with three subsets, but as this is a fairly small dataset, we will be keeping cross-validation in mind, and also therefore, we are only going for those two subsets).\n\nThe divisions we will be using when splitting the dataset are as follows:\n- 80% of `df` ----> `train`\n- 20% of `df` ----> `test`","metadata":{}},{"id":"1ddda7e9-6082-4b34-9ba5-0cf8523aca19","cell_type":"code","source":"# Train/valid/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Test shape:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:13:17.461112Z","iopub.execute_input":"2025-09-03T11:13:17.461984Z","iopub.status.idle":"2025-09-03T11:13:17.468962Z","shell.execute_reply.started":"2025-09-03T11:13:17.461954Z","shell.execute_reply":"2025-09-03T11:13:17.467749Z"}},"outputs":[{"name":"stdout","text":"Train shape: (201, 13)\nTest shape: (51, 13)\n","output_type":"stream"}],"execution_count":31},{"id":"cf51a0ab-7dcf-4161-b31b-69cde542b5cd","cell_type":"markdown","source":"### Scaling features\n\nWhen preprocessing a dataset for model training, it is good practice to scale and standardise our features, as often in many datasets, there are some features that have entries measured in different units. As in the case of our dataset, some such features include\n- Age: 20â€“80 (years)\n- Weight: 100â€“250 (lbs)\n- Height: 60â€“80 (inches)\n- Circumferences: 10â€“120 (cm)\n\nModels that rely on distances, dot products and gradient descent can get affected by this, and as we will be experimenting with a lot of regression models, we will be putting this to practice.","metadata":{}},{"id":"49fd80b3-722e-4f24-8ac3-52a945b6b537","cell_type":"code","source":"# Scale the data\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\nX_test_scaled = pd.DataFrame(scaler.transform(X_test))\n\n# Adding back the columns (as they were removed when scaling)\nX_train_scaled.columns = X_train.columns\nX_test_scaled.columns = X_test.columns\n\nX_train_scaled.head()\n#X_test_scaled.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:29:45.358140Z","iopub.execute_input":"2025-09-03T11:29:45.358771Z","iopub.status.idle":"2025-09-03T11:29:45.377770Z","shell.execute_reply.started":"2025-09-03T11:29:45.358741Z","shell.execute_reply":"2025-09-03T11:29:45.376973Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"        Age    Weight    Height      Neck     Chest   Abdomen       Hip  \\\n0 -0.517891 -0.405512  0.180513 -0.410457 -0.913066 -0.613129 -0.613767   \n1  0.664124  0.713217  0.499346  0.827925  0.819871  0.692826 -0.027908   \n2 -0.123886  1.608201 -0.074553  1.254953  2.470288  1.998780  1.070576   \n3  0.427721 -1.032001 -0.074553 -1.349919 -0.924855 -1.082897 -0.511242   \n4 -1.621106  1.017512 -0.074553  1.254953  1.208898  0.739802  0.938758   \n\n      Thigh      Knee     Ankle    Biceps   Forearm     Wrist  \n0 -0.083661 -1.030530 -0.380167  0.211451 -0.187997 -1.220435  \n1 -0.396150 -0.235461 -0.634879  1.298621  0.748065  0.742897  \n2  0.834275  0.726992 -0.762235  0.924906  0.994397 -0.893213  \n3 -1.060189 -0.863147 -0.953269 -0.264186 -1.173325 -0.456917  \n4  1.791273  0.936221  1.020749  0.415295  0.501733  0.197527  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Weight</th>\n      <th>Height</th>\n      <th>Neck</th>\n      <th>Chest</th>\n      <th>Abdomen</th>\n      <th>Hip</th>\n      <th>Thigh</th>\n      <th>Knee</th>\n      <th>Ankle</th>\n      <th>Biceps</th>\n      <th>Forearm</th>\n      <th>Wrist</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.517891</td>\n      <td>-0.405512</td>\n      <td>0.180513</td>\n      <td>-0.410457</td>\n      <td>-0.913066</td>\n      <td>-0.613129</td>\n      <td>-0.613767</td>\n      <td>-0.083661</td>\n      <td>-1.030530</td>\n      <td>-0.380167</td>\n      <td>0.211451</td>\n      <td>-0.187997</td>\n      <td>-1.220435</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.664124</td>\n      <td>0.713217</td>\n      <td>0.499346</td>\n      <td>0.827925</td>\n      <td>0.819871</td>\n      <td>0.692826</td>\n      <td>-0.027908</td>\n      <td>-0.396150</td>\n      <td>-0.235461</td>\n      <td>-0.634879</td>\n      <td>1.298621</td>\n      <td>0.748065</td>\n      <td>0.742897</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.123886</td>\n      <td>1.608201</td>\n      <td>-0.074553</td>\n      <td>1.254953</td>\n      <td>2.470288</td>\n      <td>1.998780</td>\n      <td>1.070576</td>\n      <td>0.834275</td>\n      <td>0.726992</td>\n      <td>-0.762235</td>\n      <td>0.924906</td>\n      <td>0.994397</td>\n      <td>-0.893213</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.427721</td>\n      <td>-1.032001</td>\n      <td>-0.074553</td>\n      <td>-1.349919</td>\n      <td>-0.924855</td>\n      <td>-1.082897</td>\n      <td>-0.511242</td>\n      <td>-1.060189</td>\n      <td>-0.863147</td>\n      <td>-0.953269</td>\n      <td>-0.264186</td>\n      <td>-1.173325</td>\n      <td>-0.456917</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.621106</td>\n      <td>1.017512</td>\n      <td>-0.074553</td>\n      <td>1.254953</td>\n      <td>1.208898</td>\n      <td>0.739802</td>\n      <td>0.938758</td>\n      <td>1.791273</td>\n      <td>0.936221</td>\n      <td>1.020749</td>\n      <td>0.415295</td>\n      <td>0.501733</td>\n      <td>0.197527</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"id":"222ef6d4-4d77-499a-962c-abf41a55dea8","cell_type":"markdown","source":"## Feature selection\n\nGoing back to `01_exploration.ipynb`, we went through the correlations of each feature in the dataset with the target and for this project, we will be creating two versions of these features: one with all the features in the dataset (except `'Density'`, of course) and the other with only features that show a correlation coefficient of above **0.3** with the target.\n\nWe will be using the score function `f_regression` in the `SelectKBest` class from `scikit-learn` to sort out the selected features automatically for us.","metadata":{}},{"id":"1b4643f5-235e-4ae3-8d3b-388a4da3f32a","cell_type":"code","source":"# Select top 10 features (features that have |corr| > 0.3)\nselector = SelectKBest(score_func=f_regression, k=10)\nX_train_kbest = pd.DataFrame(selector.fit_transform(X_train_scaled, y_train))\nX_test_kbest = pd.DataFrame(selector.transform(X_test_scaled))\n\n# Adding back the columns (as they were removed when scaling)\nX_train_kbest.columns = X_train.columns[selector.get_support()]\nX_test_kbest.columns = X_test.columns[selector.get_support()]\n\nX_train_kbest.head()\n#X_test_kbest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T11:51:06.344022Z","iopub.execute_input":"2025-09-03T11:51:06.344331Z","iopub.status.idle":"2025-09-03T11:51:06.363351Z","shell.execute_reply.started":"2025-09-03T11:51:06.344307Z","shell.execute_reply":"2025-09-03T11:51:06.362274Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"     Weight      Neck     Chest   Abdomen       Hip     Thigh      Knee  \\\n0 -0.405512 -0.410457 -0.913066 -0.613129 -0.613767 -0.083661 -1.030530   \n1  0.713217  0.827925  0.819871  0.692826 -0.027908 -0.396150 -0.235461   \n2  1.608201  1.254953  2.470288  1.998780  1.070576  0.834275  0.726992   \n3 -1.032001 -1.349919 -0.924855 -1.082897 -0.511242 -1.060189 -0.863147   \n4  1.017512  1.254953  1.208898  0.739802  0.938758  1.791273  0.936221   \n\n     Biceps   Forearm     Wrist  \n0  0.211451 -0.187997 -1.220435  \n1  1.298621  0.748065  0.742897  \n2  0.924906  0.994397 -0.893213  \n3 -0.264186 -1.173325 -0.456917  \n4  0.415295  0.501733  0.197527  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Weight</th>\n      <th>Neck</th>\n      <th>Chest</th>\n      <th>Abdomen</th>\n      <th>Hip</th>\n      <th>Thigh</th>\n      <th>Knee</th>\n      <th>Biceps</th>\n      <th>Forearm</th>\n      <th>Wrist</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.405512</td>\n      <td>-0.410457</td>\n      <td>-0.913066</td>\n      <td>-0.613129</td>\n      <td>-0.613767</td>\n      <td>-0.083661</td>\n      <td>-1.030530</td>\n      <td>0.211451</td>\n      <td>-0.187997</td>\n      <td>-1.220435</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.713217</td>\n      <td>0.827925</td>\n      <td>0.819871</td>\n      <td>0.692826</td>\n      <td>-0.027908</td>\n      <td>-0.396150</td>\n      <td>-0.235461</td>\n      <td>1.298621</td>\n      <td>0.748065</td>\n      <td>0.742897</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.608201</td>\n      <td>1.254953</td>\n      <td>2.470288</td>\n      <td>1.998780</td>\n      <td>1.070576</td>\n      <td>0.834275</td>\n      <td>0.726992</td>\n      <td>0.924906</td>\n      <td>0.994397</td>\n      <td>-0.893213</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.032001</td>\n      <td>-1.349919</td>\n      <td>-0.924855</td>\n      <td>-1.082897</td>\n      <td>-0.511242</td>\n      <td>-1.060189</td>\n      <td>-0.863147</td>\n      <td>-0.264186</td>\n      <td>-1.173325</td>\n      <td>-0.456917</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.017512</td>\n      <td>1.254953</td>\n      <td>1.208898</td>\n      <td>0.739802</td>\n      <td>0.938758</td>\n      <td>1.791273</td>\n      <td>0.936221</td>\n      <td>0.415295</td>\n      <td>0.501733</td>\n      <td>0.197527</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":37},{"id":"f2a66e91-d5d1-4b76-9c34-03fa05c13464","cell_type":"markdown","source":"## Saving the preprocessed data, scaler and selector\n\nNow that we have done all the required preprocessing steps, we can save the data in separate `.csv` files so that we can later import them for model training. Since we will be conducting cross-validation during training, we will also be saving both our scaler and selector alongside the data as `.pkl` files.","metadata":{}},{"id":"22565931-e0d2-492c-811a-fe58068f5642","cell_type":"code","source":"X_train_scaled.to_csv(\"/kaggle/working/X_train_all_features.csv\", index=False)\nX_test_scaled.to_csv(\"/kaggle/working/X_test_all_features.csv\", index=False)\nX_train_kbest.to_csv(\"/kaggle/working/X_train_top_features.csv\", index=False)\nX_test_kbest.to_csv(\"/kaggle/working/X_test_top_features.csv\", index=False)\ny_train.to_csv(\"/kaggle/working/y_train.csv\", index=False)\ny_test.to_csv(\"/kaggle/working/y_test.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T12:06:09.036714Z","iopub.execute_input":"2025-09-03T12:06:09.037043Z","iopub.status.idle":"2025-09-03T12:06:09.066365Z","shell.execute_reply.started":"2025-09-03T12:06:09.037022Z","shell.execute_reply":"2025-09-03T12:06:09.065566Z"}},"outputs":[],"execution_count":38},{"id":"d3d1e8c0-990a-416c-b608-6e6190c85aba","cell_type":"code","source":"# Save scaler and selector\njoblib.dump(scaler, \"/kaggle/working/scaler.pkl\")\njoblib.dump(selector, \"/kaggle/working/selector.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T12:06:11.731266Z","iopub.execute_input":"2025-09-03T12:06:11.731553Z","iopub.status.idle":"2025-09-03T12:06:11.739334Z","shell.execute_reply.started":"2025-09-03T12:06:11.731531Z","shell.execute_reply":"2025-09-03T12:06:11.738558Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/selector.pkl']"},"metadata":{}}],"execution_count":39}]}
